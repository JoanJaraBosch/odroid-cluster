#! /bin/bash


# Carreguem el script network_api.sh com a una llibreria, per 
#	poder fer servir les seves funcions
source network_lib.sh

SLURM_ETC=/etc/slurm-llnl


ip=$1
mask=$2


if [ $(munge -n | unmunge | grep ENCODE_HOST | grep \(0.0.0.0\) | wc -l) -eq 1 ]; then
	apt remove --purge munge -y
	apt install munge -y
	dd if=/home/munge.key of=/etc/munge/munge.key
	systemctl restart munge
	if [ $(munge -n | unmunge | grep ENCODE_HOST | grep \(0.0.0.0\) | wc -l) -eq 1 ]; then
		echo "Could not fix error in munge:
	ENCODE_HOST: ??? (0.0.0.0)

You can check if the problem persists with the command:
	munge -n | unmunge" 2>&1
		exit 1
	fi
fi

# Instal.lem dependencies
apt  install libfreeipmi-dev libhwloc-dev freeipmi libmunge-dev libz-dev -y

# Instal.lem slurm-wlm
apt install slurm-wlm -y

mkdir -p "$SLURM_ETC"/cgroup
cp -p /usr/share/doc/slurmd/examples/cgroup.release_common "$SLURM_ETC"/cgroup/cgroup.release_common
ln -s "$SLURM_ETC"/cgroup/cgroup.release_common "$SLURM_ETC"/cgroup/release_devices
ln -s "$SLURM_ETC"/cgroup/cgroup.release_common "$SLURM_ETC"/cgroup/release_cpuset
ln -s "$SLURM_ETC"/cgroup/cgroup.release_common "$SLURM_ETC"/cgroup/release_freezer

echo "/dev/null 
/dev/urandom 
/dev/zero 
/dev/cpu/*/* 
/dev/pts/*" > "$SLURM_ETC"/allowed_devices.conf

echo "# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster. 
# See the slurm.conf man page for more information. 
# 
ControlMachine=master 
# 
#MailProg=/bin/mail 
MpiDefault=pmi2
#MpiParams=ports=#-# 
ProctrackType=proctrack/cgroup 
ReturnToService=1 
SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
#SlurmctldPort=6817 
SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid
#SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root
StateSaveLocation=/var/spool/slurm
SwitchType=switch/none
TaskPlugin=task/cgroup
#
#
# TIMERS 
#KillWait=30 
#MinJobAge=300 
#SlurmctldTimeout=120 
#SlurmdTimeout=300 
# 
# 
# SCHEDULING 
SchedulerType=sched/builtin 
SelectType=select/cons_res 
SelectTypeParameters=CR_Socket
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageType=accounting_storage/none 
ClusterName=cluster 
#JobAcctGatherFrequency=30 
JobAcctGatherType=jobacct_gather/none 
#SlurmctldDebug=info 
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log 
#SlurmdDebug=info 
SlurmdLogFile=/var/log/slurm-llnl/slurmd.log 
# 
# 
# COMPUTE NODES 
NodeName=odroid[1-2] CPUs=4 RealMemory=1727 Sockets=4 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN 
PartitionName=main Nodes=odroid[1-2] Default=YES MaxTime=30 State=UP" > /usr/local/slurm/etc/slurm.conf

echo "CgroupAutomount=yes
CgroupReleaseAgentDir=\"/usr/local/slurm/etc/cgroup\" 
ConstrainCores=yes 
TaskAffinity=yes 
ConstrainDevices=yes 
AllowedDevicesFile=\"/usr/local/slurm/etc/allowed_devices.conf\" 
ConstrainRAMSpace=no" > "$SLURM_ETC"/cgroup.conf


chown -R slurm: "$SLURM_ETC"

if [ $(cat /etc/exports | grep "$SLURM_ETC" | wc -l) -eq 0 ]; then
	echo "${SLURM_ETC} $(calculate_network_ip $ip $mask)$(mask_to_cidr $mask)(rw,no_root_squash,no_subtree_check)" >> /etc/exports
fi

exportfs -a

# Creem un nou directori dintre de /var/spool anomenat slurm per a que slurmctld pugui fer-lo servir
mkdir /var/spool/slurm
chown slurm: /var/spool/slurm

systemctl enable slurmctld
systemctl start slurmctld
